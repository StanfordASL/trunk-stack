{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the ASL Trunk Robot documentation","text":"<p>This documentation provides a detailed guide to the setup, configuration, and use of the Trunk Robot. It is intended for internal use by the team involved in the development, deployment, and maintenance of the robot.</p> <p></p>"},{"location":"#contents","title":"Contents","text":"<ul> <li>Design: Documentation for full-system software, electrical, and mechanical design. Inlcudes BOM, CAD assets, circuit diagram, and design considerations.</li> <li>Setup: Instructions for setting up the hardware and software. This shall be referenced for repairs, upgrades, and new installations.</li> <li>Usage: Description on how to use the Trunk Robot, with details on each subcomponents and the overall data collection pipeline.</li> <li>Contributing: Guidelines for contributing to the development of the Trunk Robot project.</li> </ul>"},{"location":"3d_printing/","title":"3D Printing","text":"<p>Many of the components for the trunk robot are custom and 3D printed. All of the 3D printed components in the assembly can easily be printed on any commercial or hobbyist FDM printer. We utilized a Bambu X1C with AMS to print all 3D printed components with PLA, but other materials could alternately be used.</p>"},{"location":"3d_printing/#printer-settings","title":"Printer Settings","text":"<p>On the Bambu X1C, we used default print settings (infill of 15% and 2 wall loops) on the Bambu Cool Plate, with PLA as the primary material, PLA as the support material, and support PLA as the support/raft interface material on all prints except those listed below: - Pulleys: Printed with PLA-CF, PLA-CF support material, infill of 100% and 5 wall loops - Trunk Disks: Infill of 30% and 5 wall loops</p> <p>We found that an offset of 7 thousandths of an inch (0.18 mm) for a friction fit between manufacturer parts and 3D printed parts. For example, if a manufacturer part is friction fit into a custom 3D printed part, and the outer diameter of a manufacturer part is 1.000\", then our 3D printed part would have an internal diameter of 1.007\". This tolerance may change on other 3D printers, but we found it to be consistent across prints on the Bambu X1C.</p>"},{"location":"3d_printing/#assets","title":"Assets","text":"<p>All 3D printed assets are available in the full CAD assembly. All assets are editable and fully configurable in Fusion360. Please email mleone@stanford.edu if you need access to STL files or other filetypes.</p>"},{"location":"collecting_data/","title":"Collecting Data","text":"<p>To collect data using the Trunk robot, after setting up the robot using the motion capture and motor control instructions, the following steps can be followed.</p>"},{"location":"collecting_data/#usage","title":"Usage","text":"<p>Essentially, all you need to run is contained in: <pre><code>cd main/\nsource install/setup.bash\nros2 run executor data_collection_node \n</code></pre></p> <p>ROS arguments can be used to change the parameters. For example, to run control trajectory data collection with a control inputs file named \"control_inputs_controlled_1.csv\" and saving the observations to \"observations_controlled_1.csv\" you would run: <pre><code>ros2 run executor data_collection_node --ros-args -p data_subtype:=\"controlled\" -p results_name:=\"observations_controlled_1\" -p input_num:=1\n</code></pre></p> <p>The data will be saved in the <code>main/data/trajectories/</code> directory.</p>"},{"location":"contributing/","title":"Contributing to the ASL Trunk Robot project","text":"<p>Contributions are welcome! Here are some guidelines to follow when contributing to the project.</p>"},{"location":"contributing/#getting-started","title":"Getting started","text":"<p>Start by cloning this repository, e.g. using the following command: <pre><code>gh repo clone StanfordASL/trunk-stack\n</code></pre> where the GitHub CLI is required to use the <code>gh</code> command.</p>"},{"location":"contributing/#project-layout","title":"Project layout","text":"<p>The project is organized as follows:</p> <pre><code>trunk-stack/\n    README.md  # The project README file.\n    stack/  # The stack.\n        camera/   # The ROS2 workspace running the camera.\n        gripper/  # The ROS2 workspace enabling the gripper operation.\n        main/     # The main ROS2 workspace, incl. data collection etc.\n        mocap/    # The ROS2 workspace for interfacing with the motion capture system.\n        motors/   # The ROS2 workspace for controlling the motors.\n    docs/  # Documentation directory.\n        docs/\n            index.md         # The documentation homepage.\n            contributing.md  # This file.\n            ...              # Other markdown pages, images and other files.\n    mkdocs.yml    # The website configuration file.\n</code></pre>"},{"location":"contributing/#code-contributions","title":"Code Contributions","text":"<p>All the ROS2 packages are located within the <code>stack/</code> directory. For simplicity, this is a monorepo, meaning that all the packages are in the same repository. However, most likely contributions will be made to the <code>main/</code> package, where e.g. the controller and the main logic are located.</p>"},{"location":"contributing/#contributing-to-documentation","title":"Contributing to Documentation","text":"<p>After cloning this repository, one can make updates to the documentation by editing the files in the <code>docs/</code> directory. The documentation is built using MkDocs, a static site generator that's geared towards project documentation. Specifically, we use the Material for MkDocs theme. This should be installed using the following command: <pre><code>pip install mkdocs-material\n</code></pre> Additionally, we make use of a number of plugins, that can be installed as: <pre><code>pip install pymdown-extensions mkdocs-git-committers-plugin-2 mkdocs-git-revision-date-localized-plugin\n</code></pre> Note: The documentation is built automatically using GitHub Actions, so there is no need to build it locally. Always push to the <code>main</code> branch. In case you want to preview the updates locally, simply use: <pre><code>mkdocs serve\n</code></pre> in the main directory, and open the preview in the browser as instructed.</p>"},{"location":"electrical_design/","title":"Electrical Design","text":""},{"location":"electrical_design/#description","title":"Description","text":"<p>The trunk is actuated by 6 CIM 12V motors, each with a Talon SRX controller and an encoder. The CIM motors are powered by a 12V, 100A power supply. A 20A circuit breaker is in series with the positive terminal of each motor controller to protect from current spikes. Low level motor commands are handled with a Raspberry Pi 4, which has its own 5V power supply. CAN is the protocol used to communicate commands from the Raspberry Pi to the motor controllers, via a CANable 1.0 device. The gripper servo has its own 6V power supply. The grounds of all power supplies are connected to a common ground, which is connected to the frame. </p>"},{"location":"electrical_design/#circuit-diagram","title":"Circuit diagram","text":""},{"location":"gripper/","title":"Gripper","text":"<p>The servo-driven gripper is connected to the Raspberry Pi 4 (4GB RAM), which has Ubuntu 20.04 installed. Enabling the gripper is optional. </p>"},{"location":"gripper/#usage","title":"Usage","text":"<p>To start the gripper, open a terminal and run: <pre><code>cd gripper/\nsource install/setup.bash\nros2 run servo_control servo_control_node\n</code></pre></p>"},{"location":"mechanical_design/","title":"Mechanical Design","text":""},{"location":"mechanical_design/#description","title":"Description","text":"<p>The ASL Trunk Robot is a low cost, highly customizable, open-source desktop soft robot platform. The Trunk is powered by 6 motors, which control 12 tendons that terminate at 3 disks along the length of the robot. Custom pretensioning mechanisms keep the antagonistc tendons in tension, and the actuation unit routes the tendons into the trunk. </p>"},{"location":"mechanical_design/#full-bom","title":"Full BOM","text":"<p>The full working bill of materials is available here . </p>"},{"location":"mechanical_design/#full-cad","title":"Full CAD","text":""},{"location":"mechanical_design/#trunk-body","title":"Trunk Body","text":"<p>The flexible body of the trunk is a standard vacuum hose, which was cut to length for our application. 3 custom 3D printed disks, which each have 12 radially symmetric channels, divide the trunk into 3 equal-length segments. At each disk, 4 tendons terminate. Each disk also has a unique arrangment of motion capture markers, so OptiTrack Motive can easily distinguish them from each other for pose estimation. A custom parallel jaw gripper, adapted from this design, is mounted on the end effector, driven by a small servo housed within the trunk body. The jaws of the gripper are easily swappable for different applications, including carrying large amounts of weight (up to 600g).</p>"},{"location":"mechanical_design/#actuation-unit","title":"Actuation Unit","text":"<p>The actuation unit routes 12 tendons from their respective pretensioning mechanisms to the trunk. The main structure is a custom 3D printed mount, which connects to the frame. 6 entry holes with 6 corresponding shafts hold 12 small pulleys which route the tendons with minimal friction and no overlap. The bottom of the actuation unit has a snap-fit attachment for the top of the trunk.</p>"},{"location":"mechanical_design/#pretensioning-mechanism-ptm","title":"Pretensioning Mechanism (PTM)","text":"<p>The pretensioning mechanism is heavily inspired by Yeshmukhametov et al., 2019. A pretensioning mechanism is necessary to drive two antagonistic cables with the same motor, such that when one is pulled by the motor, the other does not go slack. Our design consists of a \"sled\" that passively tensions a tendon using two compression springs in series on the lower linear rail.</p>"},{"location":"mechanical_design/#motor-assemblies","title":"Motor Assemblies","text":"<p>Each motor assembly is centered around a CIM-12V motor. We use the CIM-12V mount along with a custom 3D printed mount to securely attach the motor and Talon SRX controller to the frame. A custom 3D printed pulley is connected to the motor shaft using a shaft key, and the tendons are secured to the top of the pulley. The Talon encoder is mounted to the frame using a custom 3D printed mount.</p>"},{"location":"mocap/","title":"Motion Capture System","text":"<p>To obtain observations, we run the motion capture system. We assume you are readily in the <code>stack</code> subdirectory of the <code>trunk-stack</code> repository.</p>"},{"location":"mocap/#usage","title":"Usage","text":"<p>First, make sure the robot is turned on. The motion capture cameras should show numbers 1-4. The Windows laptop has to be connected to the OptiHub via USB, and be running the Motive 2 software. Then, on the main computer run the following command: <pre><code>cd mocap/\nsource install/setup.bash\nros2 launch mocap4r2_optitrack_driver optitrack2.launch.py\n</code></pre> and in a new terminal run: <pre><code>cd mocap/\nsource install/setup.bash\nros2 lifecycle set /mocap4r2_optitrack_driver_node activate\nros2 run converter converter_node\n</code></pre> You can choose whether to use markers or rigid bodies by changing the <code>type</code> parameter, i.e. <pre><code>ros2 run converter converter_node --ros-args -p type:='markers'  # or 'rigid_bodies' (default)\n</code></pre></p>"},{"location":"motors/","title":"Motors","text":"<p>The 6 Dynamixel XM540-W150-R motors are connected via a U2D2 to the Linux machine. </p>"},{"location":"motors/#usage","title":"Usage","text":"<p>REQUIRED: To set USB port latency to 1 ms instead of default 16 ms, run: <pre><code>echo 1 | sudo tee /sys/bus/usb-serial/devices/ttyUSB0/latency_timer\n</code></pre></p> <p>In the same terminal, to start the motors, run: <pre><code>cd motors/\nsource install/setup.bash\nros2 launch trunk_motors launch_motors.py\n</code></pre></p>"},{"location":"optitrack/","title":"OptiTrack System","text":"<p>To obtain groundtruth position observations of the Trunk Robot, we use a motion capture system. In particular, we use equipment purchased from OptiTrack, which is one of the most well-known providers of such systems.</p>"},{"location":"optitrack/#system-components","title":"System Components","text":"<ul> <li>4 Flex 3 motion capture cameras.</li> <li>1 OptiHub 2 for camera syncing and interfacing.</li> <li>1 Windows machine for running OptiTrack's Motive software.</li> </ul>"},{"location":"optitrack/#system-setup","title":"System Setup","text":"<p>The four cameras are installed in the bottom of the frame of the system, to ensure sufficient distance between the cameras and the Trunk body. We have found that placing the cameras too close results in excessive 'ghost markers', i.e. extraneous reflections. A minimum distance is unfortunately not provided by the manufacturer, but we have that approximately a minimum distance of 50-60 cm between the cameras and the body works well.</p> <p>Once installed in the frame, one can directly connect the cameras to the OptiHub via USB, which is itself connected via USB to the Windows machine.</p> <p>Note that the Windows machine needs to have a Hardware Key to run the Motive software.</p>"},{"location":"optitrack/#motive","title":"Motive","text":"<p>The system uses Motive 2.3.7. In this software, the marker location are captured and streamed over the network, to be picked up by ROS, as described in the ROS2 Workspaces page</p> <p>First, make sure that the markers are installed in the correct locations as indicated in the Trunk Body section. Then, verify that they are showing up in the Motive interface. For each segment, simply select all the corresponding markers and create a rigid body for each segment. Ensure that in the Streaming Pane, the OptiTrack Streaming Engine is on, and interfacing with the right network. This should also have the rigid bodies enabled for streaming, as that is what we will be using as observations moving forward.</p>"},{"location":"optitrack/#calibration","title":"Calibration","text":"<p>For accurate calibration of the system, a Calibration Wand is recommended. With the cameras being mounted sturdily to the frame, we have found that we rarely have to recalibrate.</p>"},{"location":"ros2_workspaces/","title":"ROS2 Workspaces","text":"<p>Before setting up the ROS2 workspaces needed to control the robot, please make sure you have cloned this repository: <pre><code>gh repo clone StanfordASL/trunk-stack\n</code></pre> The ROS2 code lives in the <code>stack/</code> directory.</p>"},{"location":"ros2_workspaces/#main-workspace","title":"Main Workspace","text":"<p>Note: This workspace runs on the main machine.</p> <p>Enter the <code>main/</code> folder. We primarily use ROS2 Humble, which should be sourced before doing the first build: <pre><code>source /opt/ros/humble/setup.bash\n</code></pre> For actually building the workspace, we use Colcon, i.e. run: <pre><code>colcon build\n</code></pre> in the <code>main/</code> directory and check that the build process runs without any errors.</p> General note <p>It is highly recommended to build individual packages (within a workspace) upon changes, using the syntax: <pre><code>colcon build --packages-select interfaces\n</code></pre></p>"},{"location":"ros2_workspaces/#motion-capture-workspace","title":"Motion Capture Workspace","text":"<p>Note: This workspace runs on the main machine.</p> <p>Make sure you have set up the motion capture system as described in the OptiTrack System Setup page.</p> <p>Enter the <code>mocap/</code> folder. Most code is already present, except for the <code>mocap4ros2_optitrack</code> plugin, which can be installed by following the instructions on the respective website. For completeness, we also list the steps here.</p> <p>Enter the <code>src/</code> directory. Download the plugin's repository: <pre><code>cd src/\ngit clone https://github.com/MOCAP4ROS2-Project/mocap4ros2_optitrack.git\n</code></pre> Install dependencies: <pre><code>rosdep install --from-paths src --ignore-src -r -y\nvcs import &lt; mocap4ros2_optitrack/dependency_repos.repos\n</code></pre> Now, make sure that configuration file, located in <code>mocap4ros2_optitrack/mocap4ros2_optitrack_driver/config/</code>, has the correct information. Specifically, the <code>server_address</code> should be equal to the address that Motive is streaming to (see the Local Interface entry in the Streaming pane in Motive), and <code>local_address</code> should be the address of the main machine, that will be running this workspace.</p> <p>Once that information is entered correctly, compile the workspace: <pre><code>cd ..\ncolcon build --symlink-install\n</code></pre> Do make sure that ROS2 Humble is sourced again before building. Certain warnings can come up but may be ignored.</p> <p>Then, check that the Optitrack configuration works fine and is connected by running it once: <pre><code>source install/setup.bash\nros2 launch mocap4r2_optitrack_driver optitrack2.launch.py\n</code></pre> This should say \"Configured!\" as a last message. As the driver node is a lifecycle node, you should transition to activate by running in a separate terminal: <pre><code>source install/setup.bash\nros2 lifecycle set /mocap4r2_optitrack_driver_node activate\n</code></pre> which should return \"Transitioning successful\".</p>"},{"location":"ros2_workspaces/#motor-control-workspace","title":"Motor Control Workspace","text":"<p>Note: This workspace runs on the Raspberry Pi.</p> <p>For the control of the motors, we use the ros_phoenix package. Due to compatibility constraints of this package, we use ROS2 Foxy for this workspace.</p> <p>Clone the <code>trunk-stack</code> repository on the Pi and enter the <code>motors/</code> folder.</p> <p>Enter the source directory and clone the package: <pre><code>cd src/\ngit clone https://github.com/vanderbiltrobotics/ros_phoenix\n</code></pre></p> <p>Note that the launch script <code>trunk.launch.py</code> is also being version controlled, therefore ensure that is placed in its respective location.</p> <p>Again, build the workspace: <pre><code>cd ..\ncolcon build --symlink-install\n</code></pre></p>"},{"location":"ros2_workspaces/#camera-workspace","title":"Camera Workspace","text":"<p>Note: This workspace runs on the Raspberry Pi.</p> <p>First, change directory into the <code>camera/</code> folder.</p> <p>We closely follow this tutorial to install the camera driver, with the difference being that the Pi has Ubuntu 20.04 installed, not Raspberry Pi OS (previously Raspbian). Specifically, once ROS2 is installed, the following commands can be used to install the camera packages: <pre><code>cd src/\ngit clone --branch foxy https://gitlab.com/boldhearts/ros2_v4l2_camera.git\ngit clone --branch foxy https://github.com/ros-perception/vision_opencv.git\ngit clone --branch foxy https://github.com/ros-perception/image_common.git\ngit clone --branch foxy-devel https://github.com/ros-perception/image_transport_plugins.git\ncd ..\nrosdep install --from-paths src -r -y\ncolcon build\n</code></pre> You may need allow the camera to be accessed by the user, which can be done by adding the user to the <code>video</code> group, or by adding the following udev rule: <pre><code>sudo nano /etc/udev/rules.d/99-webcam.rules\nKERNEL==\"video[0-9]*\", MODE=\"0666\"  # add this to the file\nsudo udevadm control --reload-rules\nsudo udevadm trigger\n</code></pre></p>"},{"location":"ros2_workspaces/#gripper-workspace","title":"Gripper Workspace","text":"<p>Note: This workspace runs on the Raspberry Pi.</p> <p>The gripper workspace is the simplest workspace of all. Change directory into the <code>gripper/</code> folder.</p> <p>Make sure that the <code>pigpio</code> is installed on the Pi. If this library is not installed yet, one can find a straightforward description here. Also the associated Python package should be installed: <pre><code>pip install pigpio\n</code></pre></p> <p>Then, directly build the workspace: <pre><code>colcon build\n</code></pre></p>"},{"location":"software_design/","title":"Software Design","text":""},{"location":"software_design/#description","title":"Description","text":"<p>The software stack of the ASL Trunk Robot is primarily developed in ROS2, and can is divided into the components: main, motion capture, motor control, camera and gripper.</p> <p>We assume that several computing resources are available: a Rapsberry Pi for executing the motor control, a main linux machine for running the experiments, and a Windows machine for running the motion capture Motive software.</p>"},{"location":"software_design/#ros2-graph","title":"ROS2 Graph","text":"<p>We provide an overall architecture of the ROS2 nodes, topics and services used in the software stack. This diagram is inspired by ros2_graph.</p> <pre><code>flowchart \n\n/run_experiment_node[ /run_experiment_node ]:::main\n\n/converter_node_motors[ /converter_node motors ]:::node\n/converter_node_mocap[ /converter_node mocap ]:::node\n/ros_phoenix_node[ /ros_phoenix_node ]:::node\n%% /mocap4ros2_optitrack_node[ /mocap4ros2_optitrack_node ]:::node\n%% /v4l2_camera_node[ /v4l2_camera_node ]:::node\n/servo_control_node[ /servo_control_node ]:::node\n/mpc_solver_node[ /mpc_solver_node ]:::node\n\n/rigid_bodies([ /rigid_bodies ]):::topic\n/markers([ /markers ]):::topic\n/trunk_rigid_bodies([ /trunk_rigid_bodies ]):::topic\n/trunk_markers([ /trunk_markers ]):::topic\n/image_raw([ /image_raw ]):::topic\n/talon1_set([ /talon1/set ]):::topic\n/talon2_set([ /talon2/set ]):::topic\n/talon3_set([ /talon3/set ]):::topic\n/talon4_set([ /talon4/set ]):::topic\n/talon5_set([ /talon5/set ]):::topic\n/talon6_set([ /talon6/set ]):::topic\n%% /talon1_status([ /talon1/status ]):::topic\n%% /talon2_status([ /talon2/status ]):::topic\n%% /talon3_status([ /talon3/status ]):::topic\n%% /talon4_status([ /talon4/status ]):::topic\n%% /talon5_status([ /talon5/status ]):::topic\n%% /talon6_status([ /talon6/status ]):::topic\n/all_motors_status([ /all_motors_status ]):::topic\n/all_motors_control([ /all_motors_control ]):::topic\n\n/move_gripper[/move_gripper\\]:::service\n/mpc_solver[/mpc_solver\\]:::service\n/ik_solver[/ik_solver\\]:::service\n\n/run_experiment_node --&gt; /all_motors_control --&gt; /converter_node_motors\n/converter_node_motors --&gt; /talon1_set --&gt; /ros_phoenix_node\n/converter_node_motors --&gt; /talon2_set --&gt; /ros_phoenix_node\n/converter_node_motors --&gt; /talon3_set --&gt; /ros_phoenix_node\n/converter_node_motors --&gt; /talon4_set --&gt; /ros_phoenix_node\n/converter_node_motors --&gt; /talon5_set --&gt; /ros_phoenix_node\n/converter_node_motors --&gt; /talon6_set --&gt; /ros_phoenix_node\n%% /ros_phoenix_node --&gt; /talon1_status --&gt; /converter_node_motors\n%% /ros_phoenix_node --&gt; /talon2_status --&gt; /converter_node_motors\n%% /ros_phoenix_node --&gt; /talon3_status --&gt; /converter_node_motors\n%% /ros_phoenix_node --&gt; /talon4_status --&gt; /converter_node_motors\n%% /ros_phoenix_node --&gt; /talon5_status --&gt; /converter_node_motors\n%% /ros_phoenix_node --&gt; /talon6_status --&gt; /converter_node_motors\n/converter_node_motors --&gt; /all_motors_status --&gt; /run_experiment_node\n\n/rigid_bodies --&gt; /converter_node_mocap\n/markers --&gt; /converter_node_mocap\n/converter_node_mocap --&gt; /trunk_rigid_bodies --&gt; /run_experiment_node\n/converter_node_mocap --&gt; /trunk_markers --&gt; /run_experiment_node\n\n%% /v4l2_camera_node --&gt; /image_raw --&gt; /run_experiment_node\n/image_raw --&gt; /run_experiment_node\n\n/run_experiment_node &lt;==&gt; /move_gripper o==o /servo_control_node\n/run_experiment_node &lt;==&gt; /mpc_solver o==o /mpc_solver_node \n/run_experiment_node &lt;==&gt; /ik_solver o==o /ik_solver_node \n\nsubgraph keys[&lt;b&gt;Keys&lt;b/&gt;]\nsubgraph nodes[&lt;b&gt;&lt;b/&gt;]\ntopicb((Not connected)):::bugged\nmain_node[main]:::main\nend\nsubgraph connection[&lt;b&gt;&lt;b/&gt;]\nnode1[node1]:::node\nnode2[node2]:::node\nnode1 o-.-o|to server| service[/Service\\]:::service\nservice &lt;-.-&gt;|to client| node2\nnode1 --&gt;|publish| topic([Topic]):::topic\ntopic --&gt;|subscribe| node2\nnode1 o==o|to server| action{{Action}}:::action\naction &lt;==&gt;|to client| node2\nend\nend\n\nclassDef node opacity:0.9,fill:#2A0,stroke:#391,stroke-width:4px,color:#fff\nclassDef action opacity:0.9,fill:#66A,stroke:#225,stroke-width:2px,color:#fff\nclassDef service opacity:0.9,fill:#3B8062,stroke:#3B6062,stroke-width:2px,color:#fff\nclassDef topic opacity:0.9,fill:#852,stroke:#CCC,stroke-width:2px,color:#fff\nclassDef main opacity:0.9,fill:#059,stroke:#09F,stroke-width:4px,color:#fff\nclassDef bugged opacity:0.9,fill:#933,stroke:#800,stroke-width:2px,color:#fff\nstyle keys opacity:0.15,fill:#FFF\nstyle nodes opacity:0.15,fill:#FFF\nstyle connection opacity:0.15,fill:#FFF</code></pre>"},{"location":"software_design/#teleoperation","title":"Teleoperation","text":""},{"location":"software_design/#overview","title":"Overview","text":"<p>The trunk robot can be teleoperated by a user wearing an Apple Vision Pro. We designed an augmented reality app written in Swift which initializes a virtual 3D, 3-link spherical pendulum overlayed on the real-world view of the user. Once the virtual trunk is initialized, the user can calibrate the position and orientation of the virtual trunk to the hardware system. After calibration, the user can look at one of the disks on the trunk, which then lights up to denote its selection. The user can pinch their thumb and forefinger to select the disk, then the position of the virtual disk will mirror the position of their hand. The virtual disk positions can optionally be streamed over WiFi to a ROS2 listener, which publishes the 3D positions of the 3 disks on the trunk to the desired positions topic. A controller node subscribes to this topic and calculates the motor outputs necessary to attain that pose. The updated motor outputs are published to the motors, which causes the hardware trunk to mirror the virtual trunk. Streaming of desired trunk positions is done at 10Hz, and all of the other ROS2 functions run at 100Hz.</p>"},{"location":"software_design/#swift-app-design","title":"Swift App Design","text":"<p>The Apple Vision Pro teleoperation app was written in Swift 5 using XCode 16 for VisionOS 2.1.</p> <p>Our 3D assets were programmatically generated with standard hierarchical RealityKit Entities. The entities are placed into a MeshResource.Skeleton, upon which a custom IKComponent is added. A corresponding IKSolver smoothly solves the inverse kinematics of the 3 spherical pendulum joints when the position of the end effector is commanded with a gesture. The disk selection gestures are created with DragGestures. The streaming functionality for our app was heavily inspired by VisionProTeleop, using GRPC to stream disk positions over WiFi. </p> <p>Source code for the app can be found in this GitHub repository. </p>"},{"location":"telemetry_viewer/","title":"Telemetry Viewer","text":"<p>For visualizing telemetry data, such as the motor output currents, motor control temperatures, and webcam stream, we use the free foxglove tool.</p>"},{"location":"telemetry_viewer/#usage","title":"Usage","text":"<p>You can run the Foxglove server/ROS2 node by running the following command in any ROS2 workspace: <pre><code>ros2 launch foxglove_bridge foxglove_bridge.launch.xml\n</code></pre> Note that Foxglove is installed for a particular ROS2 distribution, but you can install it for any distribution, see below. Then, just open the Foxglove web interface via their website and connect. You will be able to visualize almost any data type. Finally, particular settings, such as topics to listen to, are stored and can be found in the repo.</p>"},{"location":"telemetry_viewer/#installing-foxglove","title":"Installing Foxglove","text":"<p>The only thing to do to run Foxglove is to install the WebSocket server. This can be done by running the following command: <pre><code>sudo apt install ros-$ROS_DISTRO-foxglove-bridge\n</code></pre></p>"},{"location":"teleoperation/","title":"Teleoperation","text":"<p>To collect data for visuomotor policies using the Trunk robot with teleoperation, or to just teleoperate the robot for a demonstration, first set up the robot using the motor control, motion capture, and video streaming directions, then the following steps can be followed.</p>"},{"location":"teleoperation/#usage","title":"Usage","text":"<p>Initialize a control solver node: <pre><code>cd main/\nsource install/setup.bash\nros2 run controller ik_solver_node\n</code></pre></p> <p>Start an image storing node if you want to save the recorded data: <pre><code>cd main/\nsource install/setup.bash\nros2 run streamer image_storing_node\n</code></pre></p> <p>Initialize the AVP streaming node: <pre><code>cd main/\nsource install/setup.bash\nros2 run streamer avp_streamer_node\n</code></pre></p> <p>Finally, begin an executor node: <pre><code>cd main/\nsource install/setup.bash\nros2 run executor teleop_ik_node \n</code></pre></p> <p>You can then follow the prompts in the TrunkTeleop App on the AVP to calibrate the virtual trunk to the hardware trunk, then start streaming and recording data from trunk teleoperation. </p>"},{"location":"teleoperation/#teleop-example","title":"Teleop Example","text":"<p>See the video below for an example of teleoperation in action.</p>"},{"location":"video_streaming/","title":"Video Streaming","text":"<p>The Trunk robot is equipped with a camera that can be used to stream video data to a remote computer. This can be useful for teleoperation, data collection, or other applications. The video stream is published as a ROS2 topic, which can be subscribed to by other nodes in the ROS2 network.</p>"},{"location":"video_streaming/#usage","title":"Usage","text":"<p>To start the video stream, run the following command on the PI: <pre><code>cd camera/\nsource install/setup.bash\nros2 run v4l2_camera v4l2_camera_node --ros-args -p image_size:=[1920,1080]\n</code></pre> This will start the video stream and publish the video data on the <code>/image</code> topic, and compressed video data on the <code>/image/compressed</code> topic. To alter for instance the frame rate or resolution, simply add the arguments as: <pre><code>ros2 run v4l2_camera v4l2_camera_node --ros-args -p image_size:=[1920,1080] -p framerate:=15\n</code></pre></p> <p>To subscribe to the video stream, run the following command on the main computer inside any ROS2 workspace: <pre><code>ros2 run rqt_image_view rqt_image_view\n</code></pre> and select the appropriate topic to view the video stream, e.g. <code>/image/theora</code>. This can also be viewed directly with all the other data, as described in the telemetry viewer page.</p>"},{"location":"visuomotor_rollout/","title":"Visuomotor Rollout","text":"<p>To test a visuomotor policy using the Trunk robot hardware, first set up the robot using and motor control and video streaming instructions, then the following steps can be followed.</p>"},{"location":"visuomotor_rollout/#usage","title":"Usage","text":"<p>Initialize a control solver node: <pre><code>cd main/\nsource install/setup.bash\nros2 run controller ik_solver_node\n</code></pre></p> <p>To start the visuomotor rollout, run these commands in a new terminal: <pre><code>cd main/\nsource install/setup.bash\nros2 run executor visuomotor_node\n</code></pre></p> <p>The robot hardware will then react to vision input.</p>"},{"location":"visuomotor_rollout/#example-rollout","title":"Example Rollout","text":"<p>In the following video, a ResNet18 was trained to output desired trunk pose from an image of the robot enclosure. A set of ~90 training images were collected with AVP teleoperation to have the robot point toward the red octagon. Here's the rollout seen at 8x speed:</p>"},{"location":"visuomotor_rollout/#visuomotor-policy-code","title":"Visuomotor Policy Code","text":"<p>All code for training and testing visuomotor policies is available in this GitHub repository.</p>"}]}